# Beyond Model Selection: Exploring Normalization and Feature Engineering  

While participating in the four-month-long free online course in **Machine Learning Zoomcamp** delivered by **DataTalks**, I learned that as an ML practitioner, a significant portion of time is spent constructing datasets and performing feature engineering. The process of exploring, describing, and analyzing datasets reveals inadequacies that must be addressed to improve data quality. Nothing beats a high-quality dataset.  

During the course, model selection depended on the **nature of the target variable**:  
- A **regression model** fits a **quantitative target** (e.g., predicting house prices).  
- A **classification model** determines categorical outcomes (e.g., predicting whether a student gets admitted to college based on academic and extracurricular characteristics).  
- A **neural network** solves problems such as **image classification**.  

The models covered in the course included **logistic regression, decision trees, random forests, gradient boosting, and XGBoost**, with a focus on tuning hyperparameters to optimize performance.  

However, two aspects stood out as areas worth deeper exploration: **normalization in machine learning** and **feature engineering strategies**.

---

## ðŸ”¹ The Role of Normalization in Machine Learning  

When working with machine learning models, feature scaling plays a crucial role. Some models can handle **unnormalized data**, while others perform significantly better with **normalized** features.  

**Which models require normalization?**  

- **Do not require normalization:**  
  - Decision Trees  
  - Random Forests  
  - Gradient Boosting Machines (GBM, XGBoost, LightGBM, CatBoost)  
  - Logistic Regression and Linear Regression *(without regularization)*  

- **Require normalization for optimal performance:**  
  - K-Nearest Neighbors (KNN)  
  - Support Vector Machines (SVM)  
  - Neural Networks  
  - Logistic Regression and Linear Regression *(with regularization, e.g., Lasso, Ridge)*  

### ðŸ”¹ Example: The Effect of Normalization  

Letâ€™s consider a dataset where feature values differ in scale:  

```python
import numpy as np
from sklearn.preprocessing import StandardScaler

# Sample dataset with unnormalized features
X = np.array([[1, 200], [2, 300], [3, 400]])
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Original Data:\n", X)
print("Normalized Data:\n", X_scaled)
```

Without normalization, models like SVM or KNN may give more weight to larger-scale features, leading to biased learning. Normalization ensures equal contribution from all features, improving performance and interpretability.

ðŸ”¹ Feature Engineering: Finding the Right Features

Feature engineering is one of the most challenging and rewarding parts of training a model. Adding irrelevant features can introduce noise and reduce model performance.

How do we select the most important features?

One approach is using feature importance scores from tree-based models:

```
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# Generate a sample dataset
X, y = make_classification(n_samples=100, n_features=5, random_state=42)

# Train a Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# Extract feature importance
importances = rf.feature_importances_
print("Feature Importances:", importances)
```

A baseline model can be trained using only the most important features, and additional features can be added iteratively to check for improvements in performance metrics like ROC AUC, Recall, Precision, and F1-score.

ðŸ”¹ Feature Crossing: Combining Features for Better Insights

Sometimes, new features can be derived by combining existing ones. This is known as feature crossing, where domain knowledge plays a crucial role.

For example, in an admission prediction model, instead of considering GPA and extracurriculars separately, we might create a new feature:

```
import pandas as pd

# Sample student data
df = pd.DataFrame({
    'GPA': [3.2, 3.8, 3.5],
    'Extracurriculars': [1, 0, 1]  # 1 = Yes, 0 = No
})

# Creating a feature that combines both factors
df['GPA_Extracurriculars'] = df['GPA'] * (df['Extracurriculars'] + 1)

print(df)
```

Feature crossing can improve model interpretability and performance, especially when the right domain knowledge is applied.

### Observations

	â€¢	Normalization is crucial for models that rely on distance-based calculations, ensuring features contribute equally.
	â€¢	Feature engineering can make or break a modelâ€™s success.
	â€¢	Feature importance analysis helps prioritize useful features while removing irrelevant ones.
	â€¢	Feature crossing leverages relationships between variables for better predictive power.

Exploring these aspects has deepened my understanding of how to optimize models beyond just selecting algorithms.

Would love to hear thoughts from fellow ML practitioners on their experiences with these techniques! ðŸš€

ðŸ”— Related Links
	â€¢	GitHub Repository: Beyond Model Selection
	â€¢	Live Blog Post: https://bankymondial.github.io/Beyond-Model-Selection/
